---
title: "2020/2021 Fall RMSC4002 Assignment 2"
author: "Chan Sze Yuen Syngiene 1155127616"
date: "11/13/2020"
output: pdf_document
---
The code used in this assignment is written by R.


## 1
The stock we are using here is 0151.HK, as the same stock used in assignment 1


From the assignment 1, we obtain the equaiton of the esimated voility computed by the EWMA model with parameter lambda=0.985 as follow:
$$\sigma^2_{i}=0.985\sigma^2_{i-1}+(1-0.985)u^2_{i-1}$$
We set $\sigma_0=0.02358510$, which is the fisrt 30-days volatility. Or $\sigma_0^2=0.0005562569$
In part(c), since we have 491 simulated scenario, we use the linear interpolation to to find an approximated value of the one day 0.99 VaR, Which is defined as
$$VaR(0.99)=L_{sim}(4)+(491*0.01-4)(L_{sim}(5)-L_{sim}(4))$$
After the Back Testing step with using most recent 252 days stock price, we found that the total number of exception of the 3 models are all 0, which mean these 3 method over-estimate the risk. Hence, having the smallest VaR value works better, which is hsim2 (L12 in the attached excel file).


The details of the work and the related code please refer to the attached excel file.

## 2(a)


```{r}
#import the data:
library(readr)
PokemonData=read_csv("D:\\CUHKZOOMNOTESANDSOURCE\\RMSC4002\\DATA\\Pokemon.csv")
```
data processing steps:
```{r}
#extracting the columns E to J data
Species_Strength_DataMatrix=PokemonData[,c(seq(5,5+5,1))]
#finding the correlation between different areas of strength
cor(Species_Strength_DataMatrix)
#perform PCA by corr.matrix
pcaQ2=princomp(Species_Strength_DataMatrix,cor=T)
#display the loadings of the loadings of the first six PCAs
pcaQ2$loadings[,1:6]
```
Hence, we will have the first PC is:
$$y_1=0.3898858x_{HP}+0.4392537 x_{Attack}+...+0.3354405x_{Speed}$$
and the second PC is:
$$y_2=0.08483455x_{HP}-0.01182493 x_{Attack}+...-0.66846305x_{Speed}$$


## 2(b)
The first PC telling us that the loadings are all positive and their values are similarly within the range [0.3,0.5], this PC mayber refering to the overall strength of the pokemon as higher basic stat of the pokemon the strong that pokemon is in general.


The second PC telling us that the higher defense stat that pokemon have(like Defense and Sp.Def), the lower power that pokemon have(like Attack, Sp.Atk and Speed) in general.So this PC may refer to the trade-off between defense stat and attacking stat.


## 2(c)
```{r}
s=pcaQ2$sdev #save the sd of all PC to s
s #display sd
round(s^2,4) #display variance
t=sum(s^2) #compute sum of variance of all PCs
round(s^2/t,4) #prop. of var. explained by each PC
cumsum(s^2/t) #cumulative sum of prop. of var.
```
As we can see, the first PCs just explained near 60% of the total variation of different strength area stats. So preheps we should use more PCs in order to have enough power to explain the variation. We can also check the screeplot as below:
```{r}
screeplot(pcaQ2,type="lines")
```
From the plot, there is no clear "elbow", which means using the first two PCs only is not enough to explain the total variation.


## 3(a)
```{r}
#import the data:
library(readr)
d=read_csv("D:\\CUHKZOOMNOTESANDSOURCE\\RMSC4002\\DATA\\credit.csv")
```


## 3(b)
```{r}
set.seed(27616) #my student id is 1155127616
id=sample(1:690, size=600)
d1=d[id,]
d2=d[-id,]
```


## 3(c)
```{r}
#fit the logsitc regression and show the ANOVA table of the fitted model
fit0=glm(Result~Age+Address+Employ+Bank+House+Save,data=d1,binomial)
summary(fit0)
summary(glm(Result~Address+Employ+Bank+House+Save, data=d1, binomial))
#Removed Age, which has the largest p-value
summary(glm(Result~Address+Employ+Bank+Save, data=d1, binomial))
#Remove House, which has the largest p-value
summary(glm(Result~Employ+Bank+Save, data=d1, binomial))
#Remove Address, p-value=0.24907 
#only keep the significance(p-value<0.05) variable to the new model:
fit1=glm(Result~Employ+Bank+Save,data=d1,binomial)
summary(fit1) #new model
lreg=fit1
names(lreg) # display items in lreg
pr=(lreg$fitted.values>0.5) # pr=True if fitted >0.
#classification table for this logistic regression on the training dataset d1.
table_d1=table(pr,d1$Result) # tabulation of pr & Result
print(table_d1)
```


## 3(d)
```{r}
d2hat=predict(lreg,d2) # predict the value
pt=exp(d2hat)/(1+exp(d2hat)) #Calculate the probability under the def. of log-odd ratio
Prediction=(pt>0.5) #Prediction
table_d2=table(Prediction,d2$Result) #Display the result
```


## 3(e)
```{r}
##dataset d1 first

#Precision of the model using training dataset d1
Precision_d1=(table_d1[1,1])/sum(table_d1[1,])
print(Precision_d1) 
#Recall of the model using training dataset d1
Recall_d1=(table_d1[1,1])/sum(table_d1[2,1]+table_d1[1,1])
print(Recall_d1)
#F1 score of the model using training dataset d1
F1_d1=(2*Precision_d1*Recall_d1)/(Precision_d1+Recall_d1)
print(F1_d1)

##Now we consider the dataset d2
#Precision of the model using training dataset d1
Precision_d2=(table_d2[1,1])/sum(table_d2[1,])
print(Precision_d2) 
#Recall of the model using training dataset d1
Recall_d2=(table_d2[1,1])/sum(table_d2[2,1]+table_d2[1,1])
print(Recall_d2)
#F1 score of the model using training dataset d1
F1_d2=(2*Precision_d2*Recall_d2)/(Precision_d2+Recall_d2)
print(F1_d2)

##Comparing different measure of accuracy tools using d1 and d2!
Precision_d1>Precision_d2 
#Precision of model using d1 is higher than d2
Recall_d1>Recall_d2
#Recall of model using d1 is higher than d2
F1_d1>F1_d2
#F1 score of model using d1 is higher than d2
#Hence, The overall accuracy of model using d1 is higher than d2
```


## 3(f)
```{r}
ysort_d1<-d1$Result[order(lreg$fit,decreasing=T)] # sort y
n<-length(ysort_d1) # length of ysort
perc1_d1<-cumsum(ysort_d1)/(1:n) # cumulative. perc.
plot(perc1_d1,type="l", col="blue") #plot perc.
abline(h=sum(d1$Result)/n) # add baseline
yideal_d1 <- c(rep(1,sum(d1$Result)),rep(0,length(d1$Result)-sum(d1$Result)))
# the ideal case
perc_ideal_d1 <- cumsum(yideal_d1)/(1:n)
# compute cumulative percentage of ideal case
lines(perc_ideal_d1, type="l", col="red") # plot ideal case
```


## 3(g)
```{r}
##continuous:
plot(perc1_d1,type="l", col="blue")
lines(perc_ideal_d1, type="l", col="red")
ysort_d2<-d2$Result[order(pt,decreasing=T)] # sort y
n<-length(ysort_d2) # length of ysort
perc1_d2<-cumsum(ysort_d2)/(1:n) # cumulative. perc.
lines(perc1_d2,type="l", col="green") #plot perc.
yideal_d2 <- c(rep(1,sum(d2$Result)),rep(0,length(d2$Result)-sum(d2$Result)))
# the ideal case
perc_ideal_d2 <- cumsum(yideal_d2)/(1:n)
# compute cumulative percentage of ideal case
lines(perc_ideal_d2, type="l", col="orange") # plot ideal case
legend("bottomleft", 
legend=c("Actual life chart using d1",
         "Ideal life chart using d1",
         "Actual life chart using d2",
         "Ideal life chart using d2"),
       col=c("blue", "red" ,"green" ,"orange"),
lty=1:1, cex=0.5)
```


## 3(h)
There is more space between the ideal line with actual d1 dataset line, so there is a higher error in d1 dataset compared to the actual d2 dataset.


The accuracy of the d2 dataset is higher in this case.


## 4(a)
Sorry that I cannot find a suitable dataset for this question(they are all too big and too messy for the data cleaning...really sorry about that). So I will create a 100x20 data matrix R with row equals to the 20 user (say,u1,u2,...,u100) and the colume equals to the 20 movie (say, m1,m2,...,m20). The ranking will be randomly generated using uniform discrete distribuiton,U[0,1,2,3,4,5].
```{r}
set.seed(27616) #for fixing the data matrix
R=matrix(c(sample(1:5,1000*20,replace=T)),nrow=100,byrow=T) #rating matrix
head(R) #show the head of our data matrix R
SVD_R=svd(R,nv=ncol(R)) #singular value decomposition of rating matrix
```


## 4(b)
We set the latent features be the 10 in this case(assuming there is total 10 types of movie)
```{r}
p=10
```

## 4(c)
```{r}
P=(SVD_R$u)%*%rbind(diag(p),matrix(c(rep(0,(100-p)*p)),nrow=90))
Q=t((SVD_R$v))%*%rbind(diag(p),matrix(c(rep(0,(200-p)*p)),nrow=190))
```


## 4(d)
P and Q is the singular value decomposition of the data matrix R,$R\approx PQ^T$.


We can approximate the rating matrix R by just collecting R and Q, they are lower dimesion compared to the rating matrix R. We can lower the cost of the data saving by this decomposition process.

## 4(e)
```{r}
Rhat=P%*%t(Q)
head(Rhat)
```


## 4(f)
```{r}
mse.q=sum(as.vector(Rhat-R)^2)
print(mse.q)
```
The result is clearly not reasonable as the mse is to big. Also it is because the rating matrix is just simulated by uniform distribuiton. Also, the reason behind this is because the feature is selected wrongly.


## 5(a)
Notice that Ab should be a 4x1 vector
$$||Ab-c||^2=(Ab-c)(Ab-c)^T$$
to minimize the square norm, we rewrite the form to have $A^T(A\vec{b}-\vec{c})=0$,or
$$A^TA\vec{b}=A^T\vec{c}$$
where $\vec{b}$ is the vector that we want to minimize the norm. Hence, we have the following result:
$$\vec{b}=(A^TA)^{-1}A^T\vec{c}$$
Let solve $\vec{b}$ in the following code
```{r}
A=matrix(c(1,0,2,1,1,0,0,2,1,2,1,1),byrow=T,nrow=4)
c=c(2,1,1,3)
b=solve(t(A)%*%A)%*%t(A)%*%c ; print(b)
```


## 5(b)
$$||Ab-c||^2+\lambda||b||^2=(Ab-c)(Ab-c)^T+\lambda b b^T$$
After some tedious calculation, we have
$$b=(A^TA+\lambda I)^{-1}(A^T)c$$
Hence if we are given $\lambda=0.2$,
```{r}
lambda=0.2
b=solve((t(A)%*%A+0.2*diag(3)))%*%(t(A))%*%c
b
```


## 6(a)
By Q5, we can directly calculate the beta vector by minimizing the least square$||Ab-c||$
```{r}
#inputing data
A=matrix(
c(1,rep(0,9),1,rep(0,4),
rep(0,2),1,rep(0,7),1,rep(0,4),
rep(0,4),1,rep(0,5),1,rep(0,4),
rep(0,6),1,rep(0,3),1,rep(0,4),
rep(0,7),1,rep(0,2),1,rep(0,4),
1,rep(0,9),rep(0,1),1,rep(0,3), 
rep(0,1),1,rep(0,8),rep(0,1),1,rep(0,3),
rep(0,2),1,rep(0,7),rep(0,1),1,rep(0,3),
rep(0,5),1,rep(0,4),rep(0,1),1,rep(0,3),
rep(0,8),1,rep(0,1),rep(0,1),1,rep(0,3),
1,rep(0,9),rep(0,2),1,rep(0,2), 
rep(0,1),1,rep(0,8),rep(0,2),1,rep(0,2),
rep(0,3),1,rep(0,6),rep(0,2),1,rep(0,2),
rep(0,6),1,rep(0,3),rep(0,2),1,rep(0,2),
rep(0,7),1,rep(0,2),rep(0,2),1,rep(0,2),
rep(0,8),1,rep(0,1),rep(0,2),1,rep(0,2),
rep(0,9),1,rep(0,2),1,rep(0,2),
rep(0,3),1,rep(0,6),rep(0,3),1,rep(0,1),
rep(0,4),1,rep(0,5),rep(0,3),1,rep(0,1),
rep(0,5),1,rep(0,4),rep(0,3),1,rep(0,1),
rep(0,6),1,rep(0,3),rep(0,3),1,rep(0,1),
rep(0,8),1,rep(0,1),rep(0,3),1,rep(0,1),
rep(0,9),1,rep(0,3),1,rep(0,1),
rep(0,1),1,rep(0,8),rep(0,4),1,
rep(0,2),1,rep(0,7),rep(0,4),1,
rep(0,3),1,rep(0,6),rep(0,4),1,
rep(0,4),1,rep(0,5),rep(0,4),1,
rep(0,5),1,rep(0,4),rep(0,4),1,
rep(0,7),1,rep(0,2),rep(0,4),1,
rep(0,9),1,rep(0,4),1),
byrow=T,ncol=15,nrow=30)
head(A)
c=c(5,5,4,3,5,
    4,3,2,3,2,
    4,5,3,3,4,5,5,
    1,4,3,2,4,3,
    4,3,2,5,5,5,4
)-3.67
```
We have to use regularization parameter. We may use simulation to help us find the best lambda.
In order to do so, we simulate vector c first by diiferent lambda. Then we set the lambda to be the minimum of the error of $|\hat{c}-c|$, where $\hat{c}$ is simulated by different lambda:
```{r}
temp=list()
errfun=c()
temp=list()
lambda=c(seq(-1,1,0.00001))
lambda=lambda[lambda!=0]
for (i in 1:length(lambda)){
  temp[[i]]=solve((t(A)%*%A+lambda[i]*diag(15)))%*%(t(A))%*%c
}
for (j in 1:length(lambda)){
  errfun[j]=sum(abs((A%*%temp[[j]])-c))
}
plot(errfun~lambda,ylab="error",xlab="lambda",type="l")
b.lambda=lambda[which(errfun<=min(errfun))] #best lambda
b.lambda
```
Hence, we have the optimal user bias $b^*_u$ and the optimal movie bias $b^*_i$ to be:
```{r}
b=solve((t(A)%*%A+b.lambda*diag(15)))%*%(t(A))%*%c
b
b[c(1:10),]#optimal user bias
b[-c(1:10),]#optimal movie bias
```


## 6(b)
We directly calculate the RMSE by the formula:
$$RMSE=\sqrt{\frac{\sum_{i=1}^{n}(\hat{y_i}-y_i)^2}{n}}$$
```{r}
Rhat=A%*%b+3.67
R=c(5,5,4,3,5,4,3,2,3,2,4,5,3,3,4,5,5,1,4,3,2,4,3,4,3,2,5,5,5,4)
Rhat[which(Rhat>5)]=5 #rounding
Rhat[which(Rhat<1)]=1 #rounding
RMSE=sqrt(sum((Rhat-R)^2)/length(R))
RMSE
```
## 7(a)
```{r}
d=read.csv("D:\\CUHKZOOMNOTESANDSOURCE\\RMSC4002\\DATA\\credit.csv")
set.seed(27616) #my student id is 1155127616
n=nrow(d)
id=sample(1:n,size=580)#580 random index
d1=d[id,]#training dataset d1
dim(d1) #check dimension 
d2=d[-id,]#testing dataset d2
dim(d2) #check dimension
```


## 7(b)
```{r}
library(rpart)
names(d) #Show the variables of credit.csv
#Classification tree
ctree=rpart(Result~Age+Address+Employ+Bank+House+Save,data=d1,method="class",control=rpart.control(maxdepth=3))
```


## 7(c)
```{r}
plot(ctree,asp=4,main="Credit") #Plot the branch of the tree
text(ctree,use.n=T,cex=0.6) #Add text to the tree
print(ctree) #Display the nodes
sum(d1["Result"]) #total number of accepted case
nrow(d1)-sum(d1["Result"]) #total number of rejected case
```
Rejection Rule:
1. If Bank<2.5 and Employ<1.27, then the person is rejected.
```{r}
Support=275/580 ; print(Support)
Confidence=(275-47)/275 ; print(Confidence)
Capture=(275-47)/434 ; print(Capture)
```
2. If Bank<2.5 and Employ>1.27 and Save<229.5, then the person is rejected.
```{r}
Support=127/580 ; print(Support)
Confidence=(127-50)/127 ; print(Confidence)
Capture=(127-50)/434 ; print(Capture)
```
3. If Bank>2.5 and Employ< 0.105 then the person is rejected.
```{r}
Support=11/580 ; print(Support)
Confidence=(11-4)/11 ; print(Confidence)
Capture=(11-4)/146 ; print(Capture)
```
Acceptance rules:
1. If Bank<2.5 and Employ>1.27 and Save>229.5, then the person is accepted
```{r}
Support=32/580 ; print(Support)
Confidence=(32-6)/32 ; print(Confidence)
Capture=(32-6)/434 ; print(Capture)
```
2. If Bank>2.5 and and Employ> 0.105, then the person is accepted
```{r}
Support=135/580 ; print(Support)
Confidence=(135-12)/135 ; print(Confidence)
Capture=(135-12)/146 ; print(Capture)
```


## 7(d)
```{r}
pr=predict(ctree) #probability of the sample
head(pr)
c1=max.col(pr) #classify the sample with the larger probability with 1:rejected and 2:accepted
head(c1)
table(c1,d1$Result)#classification table
Error.rate=(101+18)/580 ; print(Error.rate)
#error rate
```


## 7(e)
```{r}
pr2=predict(ctree,d2)#probability of testing data set
head(pr2)
c2=max.col(pr2)#classify the sample with the larger probability
head(c2)
table(c2,d2$Result)#classification table
Error.rate=(24+6)/(47+33+24+6) ; print(Error.rate) #error rate
```